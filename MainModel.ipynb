{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63bbf837",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install torch torchvision ultralytics\n",
    "!pip install opencv-python matplotlib\n",
    "!pip install ipywidgets # for interactive displays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997d4c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import os\n",
    "from ultralytics import YOLO\n",
    "import time\n",
    "\n",
    "# For display\n",
    "from IPython.display import display, Image as IPImage\n",
    "import ipywidgets as widgets\n",
    "\n",
    "# Check if GPU is available\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a646403",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a pretrained YOLO model (YOLOv8n by default)\n",
    "# You can choose different model sizes: n (nano), s (small), m (medium), l (large), x (extra large)\n",
    "model = YOLO('yolov8n.pt')  # this downloads the model if not already present\n",
    "\n",
    "print(f\"Model loaded: YOLOv8n\")\n",
    "print(f\"Model supports the following classes: {model.names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b072b920",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility functions for object detection\n",
    "\n",
    "def detect_objects_in_image(model, image_path, conf_threshold=0.25):\n",
    "    \"\"\"\n",
    "    Detect objects in an image using the YOLO model\n",
    "    \n",
    "    Args:\n",
    "        model: YOLO model\n",
    "        image_path: Path to the image\n",
    "        conf_threshold: Confidence threshold for detections\n",
    "        \n",
    "    Returns:\n",
    "        results: YOLO results object\n",
    "        img: Original image with bounding boxes\n",
    "    \"\"\"\n",
    "    # Run inference on the image\n",
    "    results = model(image_path, conf=conf_threshold)\n",
    "    \n",
    "    # Get the annotated image with bounding boxes\n",
    "    for r in results:\n",
    "        img = r.plot()\n",
    "        \n",
    "    return results[0], img\n",
    "\n",
    "def display_detection_results(img, results):\n",
    "    \"\"\"\n",
    "    Display detection results with annotations\n",
    "    \n",
    "    Args:\n",
    "        img: Image with bounding boxes\n",
    "        results: YOLO results object\n",
    "    \"\"\"\n",
    "    # Convert from BGR (OpenCV default) to RGB for matplotlib\n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Plot the image with detections\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.imshow(img_rgb)\n",
    "    plt.axis('off')\n",
    "    plt.title(f\"Detected {len(results.boxes)} objects\")\n",
    "    plt.show()\n",
    "    \n",
    "    # Print detection details\n",
    "    boxes = results.boxes\n",
    "    if len(boxes) > 0:\n",
    "        print(f\"\\nDetection Details:\")\n",
    "        print(\"-\" * 50)\n",
    "        for i, box in enumerate(boxes):\n",
    "            cls_id = int(box.cls.item())\n",
    "            class_name = results.names[cls_id]\n",
    "            confidence = box.conf.item()\n",
    "            x1, y1, x2, y2 = [int(coord) for coord in box.xyxy[0].tolist()]\n",
    "            print(f\"Object {i+1}: Class = {class_name}, Confidence = {confidence:.2f}, Bounding Box = [{x1}, {y1}, {x2}, {y2}]\")\n",
    "    else:\n",
    "        print(\"No objects detected.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ddcdd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test detection on a sample image from URL\n",
    "\n",
    "# Download a test image\n",
    "!curl -o test_image.jpg https://ultralytics.com/images/zidane.jpg\n",
    "\n",
    "# Run detection\n",
    "sample_path = \"test_image.jpg\"\n",
    "results, annotated_img = detect_objects_in_image(model, sample_path)\n",
    "\n",
    "# Display results\n",
    "display_detection_results(annotated_img, results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33de2de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For uploading your own images (run this in Google Colab)\n",
    "# If you're using a local Jupyter notebook, you can use other methods to select images\n",
    "\n",
    "def upload_and_detect():\n",
    "    \"\"\"\n",
    "    Function to upload an image and run object detection\n",
    "    Note: This works best in Google Colab\n",
    "    \"\"\"\n",
    "    try:\n",
    "        from google.colab import files\n",
    "        uploaded = files.upload()\n",
    "        for filename in uploaded.keys():\n",
    "            print(f\"Uploaded file: {filename}\")\n",
    "            results, annotated_img = detect_objects_in_image(model, filename)\n",
    "            display_detection_results(annotated_img, results)\n",
    "    except ImportError:\n",
    "        print(\"This function works best in Google Colab.\")\n",
    "        print(\"If you're running locally, please use the next cell to specify a path to your image.\")\n",
    "\n",
    "# Uncomment to use in Colab\n",
    "# upload_and_detect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b413b7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For detecting objects in local images\n",
    "\n",
    "def detect_local_image(image_path, conf_threshold=0.25):\n",
    "    \"\"\"\n",
    "    Run detection on a local image\n",
    "    \n",
    "    Args:\n",
    "        image_path: Path to the local image\n",
    "        conf_threshold: Confidence threshold\n",
    "    \"\"\"\n",
    "    if not os.path.exists(image_path):\n",
    "        print(f\"Error: File {image_path} not found.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Running detection on {image_path}...\")\n",
    "    results, annotated_img = detect_objects_in_image(model, image_path, conf_threshold)\n",
    "    display_detection_results(annotated_img, results)\n",
    "    \n",
    "    return results, annotated_img\n",
    "\n",
    "# Example usage (uncomment and change path to your image)\n",
    "# image_path = \"path/to/your/image.jpg\"  # Change this to your image path\n",
    "# detect_local_image(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086f4a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For processing video\n",
    "\n",
    "def process_video(video_path, output_path='output_video.mp4', conf_threshold=0.25, save_video=True, display_frames=False):\n",
    "    \"\"\"\n",
    "    Process a video file for object detection\n",
    "    \n",
    "    Args:\n",
    "        video_path: Path to input video\n",
    "        output_path: Path to save output video\n",
    "        conf_threshold: Confidence threshold for detection\n",
    "        save_video: Whether to save the processed video\n",
    "        display_frames: Whether to display frames during processing (slows down execution)\n",
    "    \"\"\"\n",
    "    if not os.path.exists(video_path):\n",
    "        print(f\"Error: Video file {video_path} not found.\")\n",
    "        return\n",
    "    \n",
    "    # Open the video file\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Error: Could not open video {video_path}\")\n",
    "        return\n",
    "    \n",
    "    # Get video properties\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    \n",
    "    print(f\"Video properties: {width}x{height} at {fps} fps, {total_frames} frames\")\n",
    "    \n",
    "    # Initialize video writer if saving\n",
    "    if save_video:\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "        out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "    \n",
    "    # Process the video\n",
    "    frame_count = 0\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "                \n",
    "            # Process frame\n",
    "            results = model(frame, conf=conf_threshold)\n",
    "            annotated_frame = results[0].plot()\n",
    "            \n",
    "            # Save frame\n",
    "            if save_video:\n",
    "                out.write(annotated_frame)\n",
    "            \n",
    "            # Display frame\n",
    "            if display_frames and frame_count % 5 == 0:  # Show every 5th frame\n",
    "                img_rgb = cv2.cvtColor(annotated_frame, cv2.COLOR_BGR2RGB)\n",
    "                plt.figure(figsize=(10, 6))\n",
    "                plt.imshow(img_rgb)\n",
    "                plt.axis('off')\n",
    "                plt.title(f\"Frame {frame_count}/{total_frames}\")\n",
    "                plt.show()\n",
    "            \n",
    "            # Update progress\n",
    "            frame_count += 1\n",
    "            if frame_count % 50 == 0:\n",
    "                elapsed = time.time() - start_time\n",
    "                fps_processing = frame_count / elapsed\n",
    "                estimated_total = elapsed * (total_frames / frame_count)\n",
    "                remaining = estimated_total - elapsed\n",
    "                print(f\"Processed {frame_count}/{total_frames} frames ({frame_count/total_frames*100:.1f}%) - {fps_processing:.1f} fps - ETA: {remaining:.1f}s\")\n",
    "    \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Processing interrupted\")\n",
    "    \n",
    "    finally:\n",
    "        # Release resources\n",
    "        cap.release()\n",
    "        if save_video:\n",
    "            out.release()\n",
    "        \n",
    "        print(f\"Processing complete: {frame_count}/{total_frames} frames processed\")\n",
    "        if save_video and os.path.exists(output_path):\n",
    "            print(f\"Output saved to {output_path}\")\n",
    "\n",
    "# Example usage (uncomment and change path to your video)\n",
    "# video_path = \"path/to/your/video.mp4\"  # Change this to your video path\n",
    "# process_video(video_path, display_frames=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d38e6556",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Real-time object detection with webcam\n",
    "\n",
    "def detect_webcam(camera_id=0, conf_threshold=0.25, save_video=False, output_path='webcam_output.mp4'):\n",
    "    \"\"\"\n",
    "    Run real-time object detection using webcam\n",
    "    \n",
    "    Args:\n",
    "        camera_id: Webcam ID (usually 0 for built-in webcam)\n",
    "        conf_threshold: Confidence threshold\n",
    "        save_video: Whether to save the output video\n",
    "        output_path: Path to save output video if save_video is True\n",
    "    \"\"\"\n",
    "    # Initialize webcam\n",
    "    cap = cv2.VideoCapture(camera_id)\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Error: Could not open webcam with ID {camera_id}\")\n",
    "        return\n",
    "    \n",
    "    # Get webcam properties\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = 30  # Target FPS\n",
    "    \n",
    "    # Initialize video writer if saving\n",
    "    out = None\n",
    "    if save_video:\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "        out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "    \n",
    "    print(\"Webcam object detection started. Press 'q' to quit.\")\n",
    "    \n",
    "    try:\n",
    "        while True:\n",
    "            # Read frame\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            \n",
    "            # Run detection\n",
    "            results = model(frame, conf=conf_threshold)\n",
    "            annotated_frame = results[0].plot()\n",
    "            \n",
    "            # Save frame if requested\n",
    "            if save_video and out is not None:\n",
    "                out.write(annotated_frame)\n",
    "            \n",
    "            # Display the frame\n",
    "            cv2.imshow(\"YOLO Object Detection\", annotated_frame)\n",
    "            \n",
    "            # Break the loop on 'q' press\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "    \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Webcam detection interrupted\")\n",
    "        \n",
    "    finally:\n",
    "        # Release resources\n",
    "        cap.release()\n",
    "        if save_video and out is not None:\n",
    "            out.release()\n",
    "        cv2.destroyAllWindows()\n",
    "        print(\"Webcam detection stopped\")\n",
    "        if save_video:\n",
    "            print(f\"Output saved to {output_path}\")\n",
    "\n",
    "# Example usage (uncomment to run)\n",
    "# detect_webcam(camera_id=0)  # Use default webcam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d7cf6a",
   "metadata": {},
   "source": [
    "## Advanced: Custom Training\n",
    "\n",
    "You can train YOLO on your own custom dataset to detect specific objects. Here are the basic steps:\n",
    "\n",
    "1. Prepare a dataset in YOLO format\n",
    "2. Create a dataset YAML configuration file\n",
    "3. Train the model\n",
    "4. Evaluate and export the model\n",
    "\n",
    "The code cell below shows how to train a model on a custom dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0abac33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom training on your own dataset\n",
    "'''\n",
    "# Example dataset.yaml file structure\n",
    "# Save this as 'dataset.yaml'\n",
    "\n",
    "path: /path/to/dataset  # dataset root directory\n",
    "train: images/train  # train images (relative to 'path')\n",
    "val: images/val  # val images (relative to 'path')\n",
    "\n",
    "nc: 3  # number of classes\n",
    "names: ['person', 'car', 'bicycle']  # class names\n",
    "'''\n",
    "\n",
    "# Training function\n",
    "def train_custom_model(yaml_path, epochs=100, batch_size=16, img_size=640):\n",
    "    \"\"\"\n",
    "    Train a custom YOLO model\n",
    "    \n",
    "    Args:\n",
    "        yaml_path: Path to dataset YAML file\n",
    "        epochs: Number of training epochs\n",
    "        batch_size: Batch size\n",
    "        img_size: Input image size\n",
    "    \"\"\"\n",
    "    # Create a new model from YOLO\n",
    "    model = YOLO('yolov8n.pt')  # Start with pretrained model\n",
    "    \n",
    "    # Train the model\n",
    "    results = model.train(\n",
    "        data=yaml_path,\n",
    "        epochs=epochs,\n",
    "        batch=batch_size,\n",
    "        imgsz=img_size,\n",
    "        patience=50,  # Early stopping patience\n",
    "        save=True,  # Save checkpoints\n",
    "        device='0' if torch.cuda.is_available() else 'cpu'\n",
    "    )\n",
    "    \n",
    "    print(\"Training complete!\")\n",
    "    return model\n",
    "\n",
    "# Usage example\n",
    "# yaml_path = \"path/to/your/dataset.yaml\"  # Change to your dataset YAML path\n",
    "# trained_model = train_custom_model(yaml_path, epochs=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71b3fc3",
   "metadata": {},
   "source": [
    "## Conclusion and Next Steps\n",
    "\n",
    "Congratulations! You've now set up a complete YOLO object detection workflow. You can:\n",
    "\n",
    "1. Detect objects in images\n",
    "2. Process videos for object detection\n",
    "3. Use your webcam for real-time detection\n",
    "4. Train custom models for specific use cases\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Try different YOLO models (yolov8s.pt, yolov8m.pt, etc.) for different speed/accuracy tradeoffs\n",
    "- Experiment with confidence thresholds\n",
    "- Create a custom dataset for your specific needs\n",
    "- Deploy your model to edge devices or cloud services\n",
    "\n",
    "For more information, visit the [Ultralytics YOLOv8 documentation](https://docs.ultralytics.com/)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
